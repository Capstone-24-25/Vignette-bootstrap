---
title: "Decision Tree"
author: "Final Project"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
  pdf_document:
    toc: true
    latex_engine: xelatex
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	tinytex.verbose = TRUE,
	fig.width = 4,
	fig.height = 3,
	fig.pos = "center"
)
```

# Load library and dataset
```{r}
library(dplyr)
library(tidymodels)
library(tidyverse)
library(recipes)
library(ggplot2)
library(mosaic)
library(graphics)
library(rpart)
library(caret)
library(gridExtra)
load("../data/Obesity_sample.RData")
```

```{r}
Obesity_sample %>% head()
```

# Decision Tree Without Bootstrapping

## Partition training and testing 
```{r}
set.seed(1234)
partitions <- Obesity_sample %>%
  initial_split(prop = 0.8, strata = NObeyesdad)
Obesity_train <- training(partitions)
Obesity_test <- testing(partitions)


Obesity_folds <- vfold_cv(Obesity_train , v = 5, strata = NObeyesdad)
```

## Data Visualization

Firstly, we see the distribution of our response variable in training data set:

```{r, fig.cap = "Obesity Status without Bootstrap"}
ggplot(Obesity_train, aes(x = NObeyesdad )) +
  geom_bar(fill = "steelblue", color = "black", alpha = 0.7) +
  labs(x = "Obesity Status", 
       y = "Frequency")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Recipe

We are going to use all predictors to predict obesity status `NObeyesdad`.

```{r}
# Creating Recipe using all variables
Obesity_recipe <-recipe(NObeyesdad ~ ., data = Obesity_train)%>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```

## Creating Decision Tree Model

We create a decision tree tuning the hyperparameter `cost_complexity`
```{r}
# Tuning cost_complexity
Obesity_tree <- decision_tree(cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("classification")

# Creating Decision Tree Workflow
Obesity_workflow <- workflow() %>% 
  add_model(Obesity_tree) %>% 
  add_recipe(Obesity_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```


```{r,eval=FALSE}
# Tune the model
Obesity_tune_tree <- tune_grid(
  Obesity_workflow,
  resamples = Obesity_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

save(Obesity_tune_tree, file = "../results/Obesity_tune_tree.RData")
```

## Visualize Result
```{r}
load("../results/Obesity_tune_tree.RData")

autoplot(Obesity_tune_tree)
```


```{r}
metrics<- collect_metrics(Obesity_tune_tree)
show_best(Obesity_tune_tree, metric = "roc_auc", n=1)
```

# Decision Tree with bootstrapping

## Bootstrapping

We bootstrap the training set for 1000 times.

```{r}
set.seed(2345)
boot_Obesity <- resample(Obesity_train,times = 1000)
boot_Obesity <- boot_Obesity %>% 
  select(-orig.id)
```

## Visualize Response Variable

```{r, fig.cap="Obesity Status with Bootstrap"}
ggplot(boot_Obesity, aes(x = NObeyesdad )) +
  geom_bar(fill = "steelblue", color = "black", alpha = 0.7) +
  labs(x = "Obesity Status", 
       y = "Frequency")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Compare with before bootstrapping

Here we visualize the distribution of our response variables from two training set. 

```{r, fig.cap = "Comparison of Obesity Status across training set", }
# Count 
freq_obesity <- Obesity_train %>%
  count(NObeyesdad) %>%
  mutate(Source = "Obesity_train")

freq_obesity_bt <- boot_Obesity %>%
  count(NObeyesdad) %>%
  mutate(Source = "Boot_Obesity_train")

# Combine
Obesity_matrix <- bind_rows(freq_obesity,freq_obesity_bt)

# Create a grouped barplot
ggplot(Obesity_matrix, aes(x = NObeyesdad, y = n, fill = Source)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Data set", 
       y = "Frequency",
       fill = "Source") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## Same recipe with data set replaced

```{r}
Obesity_recipe_bt <-recipe(NObeyesdad ~ ., data = boot_Obesity)%>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```

##Creating Decision Tree Workflow for bootstrapping sample

```{r}
Obesity_workflow_bt <- workflow() %>% 
  add_model(Obesity_tree) %>% 
  add_recipe(Obesity_recipe_bt)
```

## Tune the model
```{r,eval=FALSE}
Obesity_tune_tree_bt <- tune_grid(
  Obesity_workflow_bt,
  resamples = Obesity_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)
save(Obesity_tune_tree_bt, file = "../results/Obesity_tune_tree_bt.RData")
```

## Visualize Result

```{r}
load(file = "../results/Obesity_tune_tree_bt.RData")
autoplot(Obesity_tune_tree_bt)
```


```{r}
metrics_bt<- collect_metrics(Obesity_tune_tree_bt)
show_best(Obesity_tune_tree_bt, metric = "roc_auc", n=1)
```

Here, as we expected the performance, the performance is similar. Actually, in our case, the metric for these two model is exactly the same, which possibly attribute to the very similar training set we have. Additionally, we are using a **decision tree** purely for learning purpose, which, a simple model, might be less sensitive to the change in data set.

# Conclusion

We then fit our model with the best **cost_complexity** and predict on our test set.

## Prediction with two models

```{r}

best_complexity <- select_best(Obesity_tune_tree)

Ori_Obesity_tree_final <- finalize_workflow(Obesity_workflow, best_complexity)

Ori_Obesity_model <- fit(Ori_Obesity_tree_final, data = Obesity_train)

Ori_Obesity_model_test <- augment(Ori_Obesity_model, 
                               Obesity_test) %>% 
  select(NObeyesdad, starts_with(".pred"))

```

```{r}
best_complexity_bt <- select_best(Obesity_tune_tree_bt)

bt_Obesity_tree_final <- finalize_workflow(Obesity_workflow, best_complexity_bt)

bt_Obesity_model <- fit(bt_Obesity_tree_final, data = boot_Obesity)

bt_Obesity_model_test <- augment(bt_Obesity_model, 
                               Obesity_test) %>% 
  select(NObeyesdad, starts_with(".pred"))
```


```{r, fig.width=6,fig.height=4}
roc_ori <- roc_curve(Ori_Obesity_model_test, truth = NObeyesdad, .pred_Insufficient_Weight:.pred_Overweight_Level_II)
roc_bt <- roc_curve(bt_Obesity_model_test, truth = NObeyesdad, .pred_Insufficient_Weight:.pred_Overweight_Level_II) 

grid.arrange(
  autoplot(roc_ori), 
  autoplot(roc_bt), 
  ncol = 2
)
```

From the **roc curve**, we could see that models trained by bootstrapped data hugs the top-left corner more than the model without bootstrapping.

```{r}
confusion_matrix <- confusionMatrix(Ori_Obesity_model_test$.pred_class, Ori_Obesity_model_test$NObeyesdad)

confusion_matrix_bt <- confusionMatrix(bt_Obesity_model_test$.pred_class, bt_Obesity_model_test$NObeyesdad)


 confusion_matrix$overall[1]
 confusion_matrix_bt$overall[1]

```

| Model         | Accuracy |
|---------------|----------------|
| Without Bootstrap   |    0.845     |
| With Bootstrap |    0.859       |

Here, we can see that after bootstrapping, the accuracy does improved a little, which alignes our expectation.



